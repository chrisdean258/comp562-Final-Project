{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jiacheng",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-T9axqZtI5R"
      },
      "source": [
        "Install tensorflow 1.X & import packages\n",
        "The neural network part is borrowed from https://github.com/MaziarMF/deep-k-means which used tf 1.X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9G11Xe26Xmm"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFIfjmqhxIhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de9976d-51d8-45a7-a0ea-2e4727c5b992"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import math\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder  \n",
        "from sklearn.preprocessing import LabelEncoder "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gktxhl3jr_9B"
      },
      "source": [
        "Process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH7Ee3QWr_ec",
        "outputId": "7924393c-063e-4951-be7b-aa344b693bd9"
      },
      "source": [
        "dataset = pd.read_csv('SpotifyFeatures.csv')\n",
        "print(dataset.shape)\n",
        "dataset[:10]\n",
        "\n",
        "# label first\n",
        "encoder = LabelEncoder()  \n",
        "key = encoder.fit_transform(dataset['key'].values)  \n",
        "key = np.array([key]).T\n",
        "mode = encoder.fit_transform(dataset['mode'].values)\n",
        "mode = np.array([mode]).T\n",
        "time_signature = encoder.fit_transform(dataset['time_signature'].values)\n",
        "time_signature = np.array([time_signature]).T\n",
        "\n",
        "# then apply one-hot coding\n",
        "encoder_oh = OneHotEncoder()\n",
        "key=encoder_oh.fit_transform(key)\n",
        "key=key.toarray()\n",
        "mode=encoder_oh.fit_transform(mode)\n",
        "mode=mode.toarray()\n",
        "time_signature=encoder_oh.fit_transform(time_signature)\n",
        "time_signature=time_signature.toarray()\n",
        "print(key, mode, time_signature)\n",
        "\n",
        "# remove unnecessary columns\n",
        "data = dataset[:]\n",
        "data = data.drop(['key', 'mode', 'time_signature', 'artist_name', 'track_name', 'track_id'],axis=1)\n",
        "data = pd.concat([data,pd.DataFrame(key)],axis=1)\n",
        "data = pd.concat([data,pd.DataFrame(mode)],axis=1)\n",
        "data = pd.concat([data,pd.DataFrame(time_signature)],axis=1)\n",
        "\n",
        "data.head()\n",
        "\n",
        "# shuffle and extract the label\n",
        "data = shuffle(data)\n",
        "index = data._stat_axis.values.tolist()\n",
        "label = data['genre']\n",
        "data = data.drop(['genre'], axis=1)\n",
        "\n",
        "# normalize\n",
        "min_max_scaler = MinMaxScaler()\n",
        "data = min_max_scaler.fit_transform(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(232725, 18)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]] [[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]] [[0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJVLyeMss4Wm"
      },
      "source": [
        "Define functions\n",
        "(borrowed from the deep k-means paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZZ4idRHTSy6"
      },
      "source": [
        "TF_FLOAT_TYPE = tf.float32\n",
        "\n",
        "def fc_layers(input, specs):\n",
        "    [dimensions, activations, names] = specs\n",
        "    for dimension, activation, name in zip(dimensions, activations, names):\n",
        "        input = tf.layers.dense(inputs=input, units=dimension, activation=activation, name=name, reuse=tf.AUTO_REUSE)\n",
        "    return input\n",
        "\n",
        "def autoencoder(input, specs):\n",
        "    [dimensions, activations, names] = specs\n",
        "    mid_ind = int(len(dimensions)/2)\n",
        "\n",
        "    # Encoder\n",
        "    embedding = fc_layers(input, [dimensions[:mid_ind], activations[:mid_ind], names[:mid_ind]])\n",
        "    # Decoder\n",
        "    output = fc_layers(embedding, [dimensions[mid_ind:], activations[mid_ind:], names[mid_ind:]])\n",
        "\n",
        "    return embedding, output\n",
        "\n",
        "def f_func(x, y):\n",
        "    return tf.reduce_sum(tf.square(x - y), axis=1)\n",
        "\n",
        "def g_func(x, y):\n",
        "    return tf.reduce_sum(tf.square(x - y), axis=1)\n",
        "\n",
        "\n",
        "def next_batch(num, data):\n",
        "    \"\"\"\n",
        "    Return a total of `num` random samples.\n",
        "    \"\"\"\n",
        "    indices = np.arange(0, data.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    indices = indices[:num]\n",
        "    batch_data = np.asarray([data[i, :] for i in indices])\n",
        "\n",
        "    return indices, batch_data\n",
        "\n",
        "class DkmCompGraph(object):\n",
        "    \"\"\"Computation graph for Deep K-Means\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ae_specs, n_clusters, val_lambda):\n",
        "        input_size = ae_specs[0][-1]\n",
        "        embedding_size = ae_specs[0][int((len(ae_specs[0])-1)/2)]\n",
        "\n",
        "        # Placeholder tensor for input data\n",
        "        self.input = tf.placeholder(dtype=TF_FLOAT_TYPE, shape=(None, input_size))\n",
        "\n",
        "        # Auto-encoder loss computations\n",
        "        self.embedding, self.output = autoencoder(self.input, ae_specs)  # Get the auto-encoder's embedding and output\n",
        "        rec_error = g_func(self.input, self.output)  # Reconstruction error based on distance g\n",
        "\n",
        "        # k-Means loss computations\n",
        "        ## Tensor for cluster representatives\n",
        "        minval_rep, maxval_rep = -1, 1\n",
        "        self.cluster_rep = tf.Variable(tf.random_uniform([n_clusters, embedding_size],\n",
        "                                                    minval=minval_rep, maxval=maxval_rep,\n",
        "                                                    dtype=TF_FLOAT_TYPE), name='cluster_rep', dtype=TF_FLOAT_TYPE)\n",
        "\n",
        "        ## First, compute the distance f between the embedding and each cluster representative\n",
        "        list_dist = []\n",
        "        for i in range(0, n_clusters):\n",
        "            dist = f_func(self.embedding, tf.reshape(self.cluster_rep[i, :], (1, embedding_size)))\n",
        "            list_dist.append(dist)\n",
        "        self.stack_dist = tf.stack(list_dist)\n",
        "\n",
        "        ## Second, find the minimum squared distance for softmax normalization\n",
        "        min_dist = tf.reduce_min(list_dist, axis=0)\n",
        "\n",
        "        ## Third, compute exponentials shifted with min_dist to avoid underflow (0/0) issues in softmaxes\n",
        "        self.alpha = tf.placeholder(dtype=TF_FLOAT_TYPE, shape=())  # Placeholder tensor for alpha\n",
        "        list_exp = []\n",
        "        for i in range(n_clusters):\n",
        "            exp = tf.exp(-self.alpha * (self.stack_dist[i] - min_dist))\n",
        "            list_exp.append(exp)\n",
        "        stack_exp = tf.stack(list_exp)\n",
        "        sum_exponentials = tf.reduce_sum(stack_exp, axis=0)\n",
        "\n",
        "        ## Fourth, compute softmaxes and the embedding/representative distances weighted by softmax\n",
        "        list_softmax = []\n",
        "        list_weighted_dist = []\n",
        "        for j in range(n_clusters):\n",
        "            softmax = stack_exp[j] / sum_exponentials\n",
        "            weighted_dist = self.stack_dist[j] * softmax\n",
        "            list_softmax.append(softmax)\n",
        "            list_weighted_dist.append(weighted_dist)\n",
        "        stack_weighted_dist = tf.stack(list_weighted_dist)\n",
        "\n",
        "        # Compute the full loss combining the reconstruction error and k-means term\n",
        "        self.ae_loss = tf.reduce_mean(rec_error)\n",
        "        self.kmeans_loss = tf.reduce_mean(tf.reduce_sum(stack_weighted_dist, axis=0))\n",
        "        self.loss = self.ae_loss + val_lambda * self.kmeans_loss\n",
        "\n",
        "        # The optimizer is defined to minimize this loss\n",
        "        optimizer = tf.train.AdamOptimizer()\n",
        "        self.pretrain_op = optimizer.minimize(self.ae_loss) # Pretrain the autoencoder before starting DKM\n",
        "        self.train_op = optimizer.minimize(self.loss) # Train the whole DKM model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbQkfFmHt1We"
      },
      "source": [
        "Initialize the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFQHkZIMvLyu",
        "outputId": "884382fc-dcbd-4e0e-8812-1412612d0540"
      },
      "source": [
        "n_samples = data.shape[0]\n",
        "n_pretrain_epochs = 50\n",
        "n_finetuning_epochs = 5\n",
        "lambda_ = 1\n",
        "batch_size = 256 # Size of the mini-batches used in the stochastic optimizer\n",
        "n_batches = int(math.ceil(n_samples / batch_size)) # Number of mini-batches\n",
        "validation = False # Specify if data should be split into validation and test sets\n",
        "pretrain = True # Specify if DKM's autoencoder should be pretrained\n",
        "annealing = False # Specify if annealing should be used\n",
        "#seeded = args.seeded # Specify if runs are seeded\n",
        "\n",
        "print(\"Hyperparameters...\")\n",
        "print(\"lambda =\", lambda_)\n",
        "\n",
        "constant_value = 1  # specs.embedding_size # Used to modify the range of the alpha scheme\n",
        "max_n = 15  # Number of alpha values to consider (constant values are used here)\n",
        "alphas = 1000*np.ones(max_n, dtype=float) # alpha is constant\n",
        "alphas = alphas / constant_value\n",
        "\n",
        "target = label\n",
        "data = data\n",
        "\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
        "config = tf.ConfigProto(gpu_options=gpu_options)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters...\n",
            "lambda = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYY5XXF5jKpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97c2d40-388d-4137-ff32-50fb1096e261"
      },
      "source": [
        "n_sample = data.shape[0]\n",
        "n_clusters = 26\n",
        "input_size = data.shape[1]\n",
        "hidden_1_size = 75\n",
        "hidden_2_size = 75\n",
        "hidden_3_size = 300\n",
        "embedding_size = n_clusters\n",
        "dimensions = [hidden_1_size, hidden_2_size, hidden_3_size, embedding_size, # Encoder layer dimensions\n",
        "              hidden_3_size, hidden_2_size, hidden_1_size, input_size] # Decoder layer dimensions\n",
        "activations = [tf.nn.relu, tf.nn.relu, tf.nn.relu, None, # Encoder layer activations\n",
        "               tf.nn.relu, tf.nn.relu, tf.nn.relu, None] # Decoder layer activations\n",
        "names = ['enc_hidden_1', 'enc_hidden_2', 'enc_hidden_3', 'embedding', # Encoder layer names\n",
        "         'dec_hidden_1', 'dec_hidden_2', 'dec_hidden_3', 'output'] # Decoder layer names\n",
        "cg = DkmCompGraph([dimensions, activations, names], n_clusters, lambda_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-a7ecbfb96e3a>:6: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkj-63pGvIp-"
      },
      "source": [
        "Prepare to train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy2FneWVvRum"
      },
      "source": [
        "Train the model\n",
        "(borrowed from the deep k-means paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuZ5nU94xo0P"
      },
      "source": [
        "with tf.Session(config=config) as sess:\n",
        "  # Initialization\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess.run(init)\n",
        "\n",
        "  # Variables to save tensor content\n",
        "  distances = np.zeros((n_clusters, n_samples))\n",
        "\n",
        "  # Pretrain if specified\n",
        "        \n",
        "  print(\"Starting autoencoder pretraining...\")\n",
        "\n",
        "  # Variables to save pretraining tensor content\n",
        "  embeddings = np.zeros((n_samples, embedding_size), dtype=float)\n",
        "\n",
        "  # First, pretrain the autoencoder\n",
        "  ## Loop over epochs\n",
        "  for epoch in range(n_pretrain_epochs):\n",
        "    print(\"Pretraining step: epoch {}\".format(epoch))\n",
        "\n",
        "    # Loop over the samples\n",
        "    for _ in range(n_batches):\n",
        "      # Fetch a random data batch of the specified size\n",
        "      indices, data_batch = next_batch(batch_size, data)\n",
        "\n",
        "      # Run the computation graph until pretrain_op (only on autoencoder) on the data batch\n",
        "      _, embedding_, ae_loss_ = sess.run((cg.pretrain_op, cg.embedding, cg.ae_loss),\n",
        "                        feed_dict={cg.input: data_batch})\n",
        "\n",
        "      # Save the embeddings for batch samples\n",
        "      for j in range(len(indices)):\n",
        "        embeddings[indices[j], :] = embedding_[j, :]\n",
        "\n",
        "  # Second, run k-means++ on the pretrained embeddings\n",
        "  print(\"Running k-means on the learned embeddings...\")\n",
        "  kmeans_model = KMeans(n_clusters=n_clusters, init=\"k-means++\").fit(embeddings)          \n",
        "\n",
        "  # The cluster centers are used to initialize the cluster representatives in DKM\n",
        "  sess.run(tf.assign(cg.cluster_rep, kmeans_model.cluster_centers_))\n",
        "\n",
        "  # Train the full DKM model\n",
        "  if (len(alphas) > 0):\n",
        "    print(\"Starting DKM training...\")\n",
        "        ## Loop over alpha (inverse temperature), from small to large values\n",
        "  for k in range(len(alphas)):\n",
        "    print(\"Training step: alpha[{}]: {}\".format(k, alphas[k]))\n",
        "  \n",
        "  # Loop over epochs per alpha\n",
        "    for _ in range(n_finetuning_epochs):\n",
        "    # Loop over the samples\n",
        "      for _ in range(n_batches):\n",
        "      #print(\"Training step: alpha[{}], epoch {}\".format(k, i))\n",
        "\n",
        "      # Fetch a random data batch of the specified size\n",
        "        indices, data_batch = next_batch(batch_size, data)\n",
        "\n",
        "      #print(tf.trainable_variables())\n",
        "      #current_batch_size = np.shape(data_batch)[0] # Can be different from batch_size for unequal splits\n",
        "\n",
        "      # Run the computation graph on the data batch\n",
        "        _, loss_, stack_dist_, cluster_rep_, ae_loss_, kmeans_loss_ =\\\n",
        "        sess.run((cg.train_op, cg.loss, cg.stack_dist, cg.cluster_rep, cg.ae_loss, cg.kmeans_loss),\n",
        "              feed_dict={cg.input: data_batch, cg.alpha: alphas[k]})\n",
        "\n",
        "      # Save the distances for batch samples\n",
        "        for j in range(len(indices)):\n",
        "          distances[:, indices[j]] = stack_dist_[:, j]\n",
        "\n",
        "  # Evaluate the clustering performance every print_val alpha and for last alpha\n",
        "    print_val = 1\n",
        "    if k % print_val == 0 or k == max_n - 1:\n",
        "      print(\"loss:\", loss_)\n",
        "      print(\"ae loss:\", ae_loss_)\n",
        "      print(\"kmeans loss:\", kmeans_loss_)\n",
        "\n",
        "                # Infer cluster assignments for all samples\n",
        "      cluster_assign = np.zeros((n_samples), dtype=float)\n",
        "      for i in range(n_samples):\n",
        "        index_closest_cluster = np.argmin(distances[:, i])\n",
        "        cluster_assign[i] = index_closest_cluster\n",
        "        cluster_assign = cluster_assign.astype(np.int64) # the clustering result we have"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMTiIeHzC1GT"
      },
      "source": [
        "Compute the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKXDdYFuG--2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5440b16-aca5-45e1-9e59-45bbce87b7ac"
      },
      "source": [
        "print(cluster_assign)\n",
        "print(len(cluster_assign))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17 22  6 ...  8 18  8]\n",
            "232725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z867_YJ5ITnf"
      },
      "source": [
        "def correct_top_n(labels, predicted, cluster_counts, top=1):\n",
        "    correct = 0\n",
        "    for label, prediction in zip(labels, predicted):\n",
        "        if label in cluster_counts[prediction][:top]:\n",
        "            correct += 1\n",
        "    return correct / len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiCzQy7nQVgt"
      },
      "source": [
        "counters = defaultdict(Counter)\n",
        "\n",
        "for predicted, actual in zip(cluster_assign, label):\n",
        "    counters[predicted][actual] += 1\n",
        "\n",
        "clust_labels = {}\n",
        "for cluster, counts in counters.items():\n",
        "    clust_labels[cluster] = [a[0] for a in counts.most_common()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjFzxvc7IU9Q"
      },
      "source": [
        "for j in range(1, 6):\n",
        "    print(j, correct_top_n(label, cluster_assign, clust_labels, top=j))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}